import pandas as pd
import random
import re
import json
import multiprocessing as mp
import psutil
import os
import logging
import requests
from pymongo import MongoClient
from bson.objectid import ObjectId
from pprint import pprint
import copy
import glob
from time import *
#import finglish
import argparse
import tqdm


#from templateTools import *


memuseme	= lambda : int(psutil.Process(os.getpid()).memory_info()[0] / 2. ** 30 * 1024)
cpuuseme	= lambda : psutil.Process(os.getpid()).cpu_percent()
cpuuse		= lambda : psutil.cpu_percent()
memuse		= lambda : psutil.virtual_memory()[2]



#username = password = name = 'online6731'
#token	= ''


#print(movie, director, sep='\n\n', end='\\n\n')


questions = []

templates = []


def find_format(data):
	if not isinstance(data, list):
		data = [data]
	if len(data) == 0:
		return 'none'
	for val in data:
		val = str(val)
		#print(val, [val.find(f) != -1 for f in ['.png', '.jpg', '.jpeg']])
		if any([val.lower().find(f) != -1 for f in ['.png', '.jpg', '.jpeg', '.gif']]):
			return 'image'
		if any([val.lower().find(f) != -1 for f in ['.mp3']]):
			return 'audio'
		if any([val.lower().find(f) != -1 for f in ['.mp4']]):
			return 'video'

		return 'text'


def isnumber(value):
	try:
		_ = float(value)
		return True
	except: pass

	return False


def islink(value):
	if isinstance(value, str):
		if value.find('http') != -1:
			return True

	return False


def mongo_to_json(list_of_objects):
	if not isinstance(list_of_objects, list):
		list_of_objects = [list_of_objects]
	
	for obj in list_of_objects:
		for key in obj:
			if str(type(obj[key])) == "<class 'bson.objectid.ObjectId'>":
				obj[key] = str(obj[key])
	return list_of_objects


def all_values(dictionary):
	for value in dictionary.values():
		if isinstance(value, dict):
			yield from all_values(value)
		else:
			yield value


def download(url, local_filename=None):
	if local_filename is None:
		local_filename = url.split('/')[-1]
	else:
		local_filename += '/' + url.split('/')[-1]

	r = requests.get(url, stream=True)
	with open(local_filename, 'wb') as f:
		for chunk in r.iter_content(chunk_size=1024): 
			if chunk:
				f.write(chunk)
				#f.flush() commented by recommendation from J.F.Sebastian
	return local_filename

 
def add_question_to_mongo(question):
	# should send auestion/new request instead of directly adding it to database
	#requests.post('localhost:3000/question/new', json={ 'question': question })
	mongo.GuessIt.question.insert(question)


def add_template_to_mongo(template):
	# should send auestion/new request instead of directly adding it to database
	#requests.post('localhost:3000/question/new', json={ 'question': question })
	mongo.GuessIt.template.insert(template)


def initialization(mode='partial'):
	logger.info('initializating ...')

	#if mode == 'full':
	#	globals()['datasets'] = [re.search('.*\\\\(.*)db\.json', file_name).group(1) for file_name in glob.glob(f'{db_directory}*.json')]
	#	logger.info(f"all datasets that were found: {', '.join(globals()['datasets'])}")
		
	for dataset in datasets:
		globals()[dataset] = None
		if mode == 'full':
			globals()[dataset + 'db'] = None
	
	for var in template_arrays:
		globals()[var] = []
	
	for var in template_dicts:
		globals()[var] = {}
	
	if mode == 'full':
		try:
			globals()['translation' + 'db'] = load_data('translation')
		except:
			logger.error(f'could not load translation db')
	
	logger.info('initialization is done.')


def load_data(dbname):
	logger.info(f'def load_data(dbname={dbname})')
	
	problems = []
	
	data = pd.DataFrame()
	
	while True:
		try:
			logger.info(f'trying to load {dbname} dataset from hard disk...')
			
			data = pd.DataFrame(json.load(open(f'{project_dir}/datasets/{dbname}db.json', encoding='utf-8')))
			
			logger.info(f'loading {dbname} dataset is done.')
			
			break
			
		except Exception as error:
			
			problems += [f'could not open dataset {dbname} from {project_dir}/datasets/ directory because {error}']

			break
			
			#logger.info(f'trying to download {dbname} dataset from server...')

			#download(f'{db_url}{dbname}db.json', 'database/')
		except:
			logger.error(f'shit happened while loading {dbname} dataset')
			
			break
	
	
	return data, problems


def used_datasets(template):
	# BUG : does not find datasetss that had been used in meddle of code. example : director[(movie.name....)] 
	
	used_dataset = []
	if 'values' not in template:
		return []
	
	joint_code = ' '.join(template['values'].values())
	while joint_code.find('db') != -1:
		if re.search('db\(([a-zA-Z]*).*\)', joint_code):
			used_dataset += [re.search('db\(([a-zA-Z]*).*\)', joint_code).group(1)]
		joint_code = joint_code[joint_code.find('db') + 2:]
	return used_dataset


def load_used_datasets(template):
	problems = []
	
	if memuseme() > 1300:
		initialization()
	
	for dataset in [re.search('.*?/([a-zA-Z]*?)db.json', address).group(1) for address in glob.glob(f'{project_dir}/datasets/*.json')]:#used_datasets(template):
		if f'{dataset}db' not in globals() or globals()[f'{dataset}db'] is None:
			globals()[dataset + 'db'], new_problems = load_data(dataset)	
			problems += new_problems
	return problems

def excluce_datasets(template, ILMIN=0, ILMAX=1):	
	for dataset in used_datasets(template):
		if globals()[dataset] is None:
			
			try:
				if len(globals()[dataset + 'db'].index)*(ILMAX - ILMIN) > 10:
					   globals()[dataset] = globals()[dataset + 'db'].sort_values('popularity', ascending=False).iloc[int(len(globals()[dataset + 'db'].index)*ILMIN):int(len(globals()[dataset + 'db'].index)*ILMAX)]
				else:
					   globals()[dataset] = globals()[dataset + 'db'].sort_values('popularity', ascending=False)
			except:
				globals()[dataset] = globals()[dataset + 'db']#.iloc[int(len(globals()[dataset + 'db'].index)*ILMIN):int(len(globals()[dataset + 'db'].index)*ILMAX)]
			
			#globals()[dataset] = globals()[dataset + 'db']
			#globals()[dataset].dropna()



def choose(items, count=None):
	#print('choose', str(items)[:20], count)
	random.shuffle(items)
	if count:
		return items[:count]
	else:
		return items[0]
	
	return items[random.randint(0, len(items))]
	
	if not items or not rand(0, len(items) - 1, count, [], False):
		return []
	
	
	
	if count == None:
		selected_indices = rand(0, len(items) - 1, 1, [], False)
	else:
		selected_indices = rand(0, len(items) - 1, count, [], False)
	
	selected_items = []
	for i in selected_indices:
		selected_items += [items[random.randint(0, len(items))]]
	
	if count == None:
		selected_items = selected_items[0]
	
	return selected_items


def make_help(data_name, data, exceptions=[], language='en'):
	#print(data_name, data, exceptions, language)
	
	#data_id = data.id
	
	chosen_helps = []
	
	helps = {
		'director': {
			'en': {
				'age'	   : "he/she is {data.age} years old",
				'movies'	: "some of his/her movies are {' , '.join(data.movies)}"
			},
			'fa': {
				'age'	   : "او {data.age} سال دارد",
				'movies'	: "تعدادي از فيلم هاي او عبارتند از : {' , '.join(data.movies)}"
			}
		},
		'actor': {
			'en': {
				'age'	   : "he/she is {data.age} years old",
				'movies'	: "some of his/her movies are {' , '.join(data.movies)}"
			},
			'fa': {
				'age'	   : "او {data.age} سال دارد",
				'movies'	: "تعدادي از فيلم هاي او عبارتند از : {' , '.join(data.movies)}"
			}
		},
		'movie': {
			'en': {
				'release_year'   : "it was released in {int(data.release_year)}",
				'imdb_rate'	  : "it's imdb rate is {data.imdb_rate}", 
				'stars'		 : "some of it's casts are {' , '.join(data.stars)}",
				'characters'	: "some of it's characters are {' , '.join(data.characters[:3])}",
				'genres'		: "it' genres are {' , '.join(data.genres)}",
				#'tag_line'		: "`choose(data.tag_line)`"
			},
			'fa': {
				'release_year'   : "اين فيلم در سال {int(data.release_year)} منتشر شد",